<!DOCTYPE html>
<div id="body-container">
    <div id="panel-container">
        <div class="left-panel">
            <div class="content">
                <h1>Random Forest Regression
                </h1>
                <p>
                    It's time to shift our gears from the linear simplicity to the ensemble complexity! Think of Random
                    Forest Regression like a weather forecast team instead of just one meteorologist. It's an ensemble
                    learning method that combines multiple decision trees (think of these as individual forecasts) to
                    make a more accurate and robust prediction. Each tree in the forest makes its own prediction, and
                    the final output is the average of all these predictions. This method helps in reducing overfitting
                    (too much reliance on the training data) and improves accuracy.
                </p>
                <p>
                    Unlike linear regression, which tries to find a straight line that best fits the data, Random Forest
                    can capture complex, nonlinear relationships between the features and the target variable. So, in
                    the context of our data - like the time series data of maximum temperature, mean temperature, and
                    others - Random Forest would analyze patterns over time, taking into account various lag features
                    and possibly even rolling averages to predict future conditions. This would be especially useful for
                    more volatile weather elements, such as precipitation, where straightforward linear predictions
                    might fall short. We'll preprocess our data to remove gaps and outliers. Let's plot some box plots
                    to find outliers.
                </p>
                <pre><code class="language-python">import pandas as pd
import plotly.graph_objects as go
df = pd.read_csv('data/madrid1.csv')
df['CET'] = pd.to_datetime(df['CET'])
df.drop(['GUSTMAX', 'CC', 'Events', 'Dew Point', 'VISMIN', 'VISMAX', 'VISMEAN'], axis=1, inplace=True)
df.set_index('CET', inplace=True)
df[['TMEAN','TMAX', 'TMIN', 'DEWMEAN', 'DEWMIN']] = df[['TMEAN','TMAX', 'TMIN', 'DEWMEAN', 'DEWMIN']].apply(lambda c: (c * 9/5) + 32)
                    df[['WINDMAX']] = df[['WINDMAX']].apply(lambda km: km * 0.621371)
                    df['PRCP'] = df['PRCP'].apply(lambda mm: mm * .0393701)
                    
fig = go.Figure()
for column in df.columns:
    fig.add_trace(go.Box(y=df[column], name=column))
                    
fig.update_layout(
title='Box Plots of all Data',
yaxis_title='Values',
showlegend=True
)
fig.show()</pre></code>
                <p>
                    Our outliers are the circles outside of the box minimum or maximum. This doesn't mean they are all
                    actually outliers. We have to use our best judgement. The Max Wind Speed outliers, considering that
                    this isn't a record of max gusts, look suspicious. We'll remove them. I'll leave it as a challenge
                    to you to find out if there's any gaps in your data in a clever way.
                </p>
                <span class="code-subtext">// Remove notable outliers and select data time range with no gaps for our
                    variables
                </span>
                <p>

                </p>
                <pre><code class="language-python">df = df[['TMIN', 'TMAX', 'TMEAN', 'DEWMEAN', 'HUMMEAN', 'MSLPMEAN', 'PRCP', 'VISMEAN', 'WINDMAX', 'WDD', 'CC']]
df = df[df['WINDMAX'] &lt;= 61]
df = df[df['MSLPMIN'] >= 970]
df = df["2008-01-01:"]
target = 'TMEAN'
predictors = df.drop(['TMAX', 'TMIN', 'TMEAN'], axis=1).columns
X = df[predictors]
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)
</pre></code>
                <p>
                    Let's start by doing what we did in the first section, and trying to predict a variable based on
                    others from the same day. We'll run our test with the exact same predictors and target, but use the
                    Random Forest Regressor model. The random_state works in the same way as during our linear
                    regression testing. Remember, we are not treating this as time series data, so we want to randomly
                    split our test and training sets. The input n_estimators dictates how many trees will be in the
                    forest. Theoretically, more decision trees will lead to better results, but also increase running
                    time. Let's try it with 10 trees,
                    then 100 trees. We also have a new metric called R2 score, which tells us what percentage of the
                    variance
                    in our target can be explained by the predictors.
                </p>
                <pre><code class="language-python">from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
trees = 10 # Change to number of trees you want to test
model = RandomForestRegressor(n_estimators=trees, random_state=44)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
# Calculate other error metrics like our other examples and plot by a graphing library of your choice</pre></code>
                <p>Even our results with 10 decision trees are better than our linear regression example. Our MAE and
                    MSE are 1.59 and 4.23 respectively, while during the same trial with linear regression, they were
                    1.81 and 5.36 respectively. Note that since we used the same random_state, the data was split in the
                    same way. With 100 trees, our prediction improves even more. With 1000 (not shown), I did not notice
                    any improvement. You can also see that our predictions do not suffer at the highest range of
                    temperature like our linear regression did. Let's try the same 10 and 100 trees for MSLP, with the
                    same predictors as before.
                </p>
                <pre><code class="language-python">target = 'MSLPMEAN'
predictors = df.drop(['MSLPMEAN'], axis=1).columns]
# Initialize and fit the model, calculate error, and graph same as we did before</pre></code>
                <p>We only notice a slight improvement from an MSE of 3.90 to 3.80 with 100 decision trees. For fun,
                    let's try to predict Mean Visibility with 100 trees
                    and all variables besides itself and other visibility measures (we dropped them earlier).
                </p>
                <pre><code class="language-python">target = 'VISMEAN'
predictors = df.drop(['VISMEAN'], axis=1).columns]
# Initialize and fit the model, calculate error, and graph same as we did before</pre></code>
                <p>The Random Forest model actually performs worse than linear regression. LR had an MAE of 1.22 and MSE
                    of 3.51, while RF had an MAE of 1.82 and MSE of 8.34. Note our R2 score is 0.28, so only 28 percent
                    of the variance in the visibility can be attributed to our predictors. Let's also use the Random
                    Forest Regressor to forecast. We'll use the same setup as our linear regression on time series. The
                    features used are noted under each graph for comparison.</p>
                <pre><code class="language-python">predictors = ['TMEAN_lag1', 'TMAX_lag1', 'TMIN_lag1', 'MSLPMEAN_lag1', 'DEWMEAN_lag1', 'HUMMEAN_lag1']
target = 'TMEAN' # This and above line are different for each graph

train = df["2010-01-01":"2014-12-31"]
test = df["2015-01-01":]

X_train = train[predictors]
y_train = train[target]
X_test = test[predictors]
y_test = test[target]

model = RandomForestRegressor(n_estimators=250, random_state=44)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)</pre></code>
                <div class="button-container">
                    <button class=selector-button onclick="loadSection(2)">&lt;</button>
                    <button class=selector-button onclick="loadSection(4)">&gt;</button>
                </div>
            </div>
        </div>
        <div class="right-panel">
            <div class="content">
                <h1>Data Visualization</h1>
                <div class=iframe-wrapper>
                    <iframe id=graphFrame>
                    </iframe>
                </div>
                <div class="button-container">
                    <button class=selector-button id="graph-rf-left" onclick="loadPreviousGraph()">&lt;</button>
                    <button class=selector-button id="graph-rf-right" onclick="loadNextGraph()">&gt;</button>
                </div>
            </div>
        </div>
    </div>
</div>